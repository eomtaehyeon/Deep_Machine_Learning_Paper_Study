# Deep/Machine Learning Paper_Study

## 개요.

    작업툴: Notion, Github, Powerpoint
    인원: 3명
    기간: 2022.04.10~2022.07.09
    내용: 
        - 딥러닝 및 머신러닝 모델에 대한 관련 논문을 읽고 블로그에 정리
        - 딥러닝 및 머신러닝 모델을 코드로 구현 및 실습. 
 ([장민지님](https://github.com/jmj3047)
, [엄태현님](https://github.com/eomtaehyeon) 
, [김영재님](https://github.com/Kimyoungjae777))

## 논문 리뷰 목록.

**- Attention is all you need**

    - 공부 기간 : 2022.04.10 ~ 
    - Tag : NLP (자연어처리)
    - Description(한줄 요약)
     : Transformer, Attention 만으로 시퀀셜 데이터를 분석하여 병렬화와 연산 속도 향상을 가능하게 한 새로운 모델 제시합니다.
    - Journal/Conference : NIPS (신경 정보처리 시스템 학회)
    - Year(출판년도) : 2017
    - Author
     : Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin 

[Transformer 논문 리뷰자 : 엄태현](https://github.com/eomtaehyeon/Deep_Machine_Learning_Paper_Study/blob/main/Transformer/Attention%20is%20all%20you%20need%20Review.md)


**- ImageNet Classification with Deep Convolutional Neural Networks**

    - 공부 기간 : 2022.04.21 ~ 2022.07.09
    - Tag : AlexNet, Computer Vision(컴퓨터비전)
    - Description(한줄 요약)
     : 기존 머신러닝 모델을 제치고 딥러닝 모델이 더 우수한 성능을 보일 수 있음을 증명한 최초의 모델
        ReLU 활성화 함수와 Dropout 의 유용함, Data Augmentation 기법을 제시
        2012년 ImageNet 대회 ILSVRC 에서 우승을 차지한 모델
    - Journal/Conference : NIPS (신경 정보처리 시스템 학회)
    - Year(출판년도) : 2012
    - Author
     : Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton

[ImageNet Classification 논문 리뷰자 : 장민지](https://jmj3047.github.io/2022/04/21/ImageNet_Classification/)

**- Improved Training of Wasserstein GANs**

    - 공부 기간 : 2022.04.22 ~ 2022.07.09
    - Tag : DCGAN, Generative Model(생성모델)
    - Description(한줄 요약)
     : 기존의 Wasserstein-GAN 모델의 weight clipping 을 대체할 수 있는 gradient penalty 방법을 제시
        hyperparameter tuning 없이도 안정적인 학습이 가능해졌음을 제시
    - Journal/Conference : NIPS (신경 정보처리 시스템 학회)
    - Year(출판년도) : 2017
    - Author
     : Ishaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, Aaron C. Courville

[GANs 논문 리뷰자 : 장민지](https://jmj3047.github.io/2022/04/22/WGAN/)

**- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**

    - 공부 기간 : 2022.04.22 ~ 
    - Tag : NLP (자연어처리)
    - Desciption(한줄요약)
     : BERT, Google에서 개발한 자연어 처리 사전 교육을 위한 변압기 기반 기계 학습 기술입니다.
    - Journal/Conference : NAACL (북미 컴퓨터 언어학 학회)
    - Year(출판년도) : 2018
    - Author 
     : Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova

[BERT 논문 리뷰자 : 엄태현](https://github.com/eomtaehyeon/Deep_Machine_Learning_Paper_Study/blob/main/BERT_REVIEW/BERT_Pre-t.md)

**- Attention is all you need**

    - 공부 기간 : 2022.05.10 ~ 2022.07.09
    - Tag : NLP (자연어처리)
    - Description(한줄 요약)
     : Attention 만으로 시퀀셜 데이터를 분석하여 병렬화와 연산 속도 향상을 가능하게 한 새로운 모델 제시.
        Seq2Seq 과 Attention 을 결합한 모델(Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
        learning to align and translate. CoRR, abs/1409.0473, 2014.)에서 한층 더 발전한 모델입니다.
        Recurrent model(재귀 구조)없이 Self-attention 만으로 구성한 첫번째 모델입니다.
        재귀 구조 제거로 모델을 병렬화(Parallelization)하여 자연 언어 처리 학습/추론 시간을 획기적으로 단축시켰습니다.
    - Journal/Conference : NIPS (신경 정보처리 시스템 학회)
    - Year(출판년도) : 2017
    - Author
     : Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin 

[Transformer 논문 리뷰자 : 장민지](https://jmj3047.github.io/2022/05/10/Attention_is_all_you_need/)

**- You Only Look Once: Unified, Real-Time Object Detection**

    - 공부 기간 : 2022.06.10 ~ 
    - Tag : Computer Vision (컴퓨터비전)
    - Desciption(한줄요약)
     : YOLO
    - Journal/Conference : CVPR (컴퓨터 비전과 패턴 인식 컨퍼런스)
    - Year(출판년도) : 2015
    - Author 
     : Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi

[YOLO 논문 리뷰자 : 엄태현](https://github.com/eomtaehyeon/Deep_Machine_Learning_Paper_Study/blob/main/YOLO_v5/YOLOv5.md)

**- Deep Embedding Learning for Text-Dependent Speaker Verification**

    - 공부 기간 : 2022.07.05 ~ 
    - Tag : Speaker Verification (화자인식)
    - Desciption(한줄요약)
     : 이 논문은 화자 검증 작업을 위한 효과적인 딥 임베딩 학습 아키텍처를 제시한다.
        널리 사용되는 잔류 신경망(ResNet) 및 시간 지연 신경망(TDNN) 기반 아키텍처와 비교하여, 두 가지 주요 개선이 제안된다.
        우리는 화자의 단기 컨텍스트 정보를 인코딩하기 위해 조밀하게 연결된 컨볼루션 네트워크(DenseNet)를 사용한다.
        양방향 주의 풀링 전략이 제안된다. 장기적인 시간적 맥락을 모델링하고 화자 정체성을 반영하는 중요한 프레임을 집계한다.
        결과는 제안된 알고리듬이 과제 1과 과제 3의 평가 세트에서 각각 8.06%, 19.70% minDCF 및 9.26%, 16.16% EERs 상대적 감소로 FFSVC2020의 공식 기준선을 능가한다는 것을 보여준다.
    - Journal/Conference : Interspeech 
    - Year(출판년도) : 2020
    - Author 
     : Peng Zhang, Peng Hu, Xueliang Zhang

[Deep Embedding Learning 논문 리뷰자 : 장민지](https://jmj3047.github.io/2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/)