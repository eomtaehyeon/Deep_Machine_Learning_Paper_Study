# BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding Review

- Conference : NAACL
- Link :

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

- Year : 2018
- ì €ì : Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
- Github :

[https://github.com/google-research/bert](https://github.com/google-research/bert)

- ë™ì˜ìƒ :

[https://vimeo.com/365139010](https://vimeo.com/365139010)

# BERTë€?

Bidirectional Encoder Representations from Transformers. 

Unlike recent language representation models.

BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.

As a result, the pre-trained BERT model can be fine tuned with just one additional output layer to create state-of-the art models for a wide range of tasks, such as question answering and language inference, without substanial task specific architecure modifications.

# í•µì‹¬ ìš”ì•½

- trasformerì˜ encoder networkë¥¼ ê¸°ë°˜ìœ¼ë¡œ, self-attentionì„ ì´ìš©í•˜ì—¬ bidirectionalí•˜ê²Œ ì–¸ì–´ íŠ¹ì„±ì„ í•™ìŠµí•©ë‹ˆë‹¤.

    - transformerë€?
    
    transformerëŠ” the mechanism of self-attention**ì„ ì±„íƒí•˜ëŠ” ë”¥ ëŸ¬ë‹ ëª¨ë¸ë¡œì„œ, ì…ë ¥ ë°ì´í„°ì˜ ê° ë¶€ë¶„ì˜ ì¤‘ìš”ì„±ì„ ì°¨ë“±ì ìœ¼ë¡œ ê°€ì¤‘ì‹œí‚¨ë‹¤. ì£¼ë¡œ ìì—°ì–´ ì²˜ë¦¬(NLP)ì™€ ì»´í“¨í„° ë¹„ì „(CV) ë¶„ì•¼ì—ì„œ ì‚¬ìš©ëœë‹¤.**
    
    - transformer background
    
    before transformers, most state-of-the-art NLP systems relied on gated RNNs, such as LSTM and gated recurrent units (GRUs), with added attention mechanisms. Transformers are built on these attention technologies without using an RNN structure, highlighting the fact that attention mechanisms alone can match the performance of RNNs with attention.
    
    transformer ì´ì „ì— ëŒ€ë¶€ë¶„ì˜ ìµœì²¨ë‹¨ NLP ì‹œìŠ¤í…œì€ LSTMê³¼ ê²Œì´íŠ¸ ë°˜ë³µ ìœ ë‹›(GRU)ê³¼ ê°™ì€ ê²Œì´íŠ¸ RNNì— ì˜ì¡´í–ˆìœ¼ë©° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì¶”ê°€ë˜ì—ˆë‹¤. ë³€ì••ê¸°ëŠ” RNN êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì´ëŸ¬í•œ ì£¼ì˜ ê¸°ìˆ ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì–´ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œë„ RNNì˜ ì„±ëŠ¥ì„ ì£¼ì˜ì™€ ì¼ì¹˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ê°•ì¡°í•œë‹¤.
    
    - Encoder
    
    Each encoder consists of two major components : a self-attention mechanism and a feed-forward neural network. The self-attention mechanism accepts input encodings from the previous encoder and weighs their relevance to each other to generate output encodings.
    
    ê° ì¸ì½”ë”ëŠ” self-attention mechanismê³¼ feed-forward neural network ì´ë¼ëŠ” ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤. ìì²´ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì´ì „ ì¸ì½”ë”ë¡œë¶€í„° ì…ë ¥ ì¸ì½”ë”©ì„ ë°›ì•„ë“¤ì´ê³  ì¶œë ¥ ì¸ì½”ë”©ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì„œë¡œ ê´€ë ¨ì„±ì„ í‰ê°€í•œë‹¤.
    
    ì°¸ì¡° : [Transformer (machine learning model) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)?msclkid=a821d608bd7b11ec9cad602d05a50e33)
    
    [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    
    ì°¸ì¡° : 
    
    [https://www.youtube.com/watch?v=mxGCEWOxfe8](https://www.youtube.com/watch?v=mxGCEWOxfe8)
    
- MLM(Masked language model)ê³¼ NSP(next sentence prediction)ë“±ì˜ pre-trainingë°©ë²•ì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤.
    - MLM
    
    In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.
    
    deep bidirectional representationì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ ì…ë ¥ í† í°ì˜ ì¼ë¶€ ë¹„ìœ¨ì„ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•œ ë‹¤ìŒ, ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡í•œë‹¤.
    
    ì°¸ì¡° : 
    
    [](https://aclanthology.org/2020.acl-main.240.pdf?msclkid=3edd2a0dbe0211ecab2f655db1333859)
    
    - NSP
    
- pre-trainingë°©ë²•ìœ¼ë¡œ feature representationì„ í•™ìŠµí•œ ë’¤ fine-tuningë§Œìœ¼ë¡œ down-stream taskë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

# Introduction & Related Work

ì €ìëŠ” ë¨¼ì € ì‚¬ì „ í›ˆë ¨ì„ í†µí•´ ìì—°ì–´ ì²˜ë¦¬ Taskì˜ ì„±ëŠ¥ì„ í–¥ìƒ ì‹œí‚¤ëŠ” PLM(pre-trained language model)ì˜ ì‚¬ë¡€ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

- Feature-based
    - ì˜ˆì‹œ)ELMo : pre-trained representation ì„ ì¶”ê°€ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ Taskì— íŠ¹í™”ëœ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.
- fine-tuning
    - ì˜ˆì‹œ) GPT : taskì— íŠ¹í™”ëœ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì†Œí™” í•˜ê³ , ì‚¬ì „ í•™ìŠµëœ íŒŒë¼ë¯¸í„° ì „ë¶€ë¥¼ fine-tuning í•©ë‹ˆë‹¤.

ë‘ ë°©ì‹ ëª¨ë‘ pre-trainingì—ì„œ ë™ì¼í•œ objectiveë¥¼ ì‚¬ìš©í•˜ê³ , unidirectional language modelì„ ê°€ì •í•©ë‹ˆë‹¤. ì´ ë•Œë¬¸ì—, ë¬¸ë§¥ì„ ë‹¨ ë°©í–¥ìœ¼ë¡œë§Œ ìŠµë“í•˜ê²Œ ë˜ì–´, ì–‘ë°©í–¥ì˜ ë¬¸ë§¥ ì´í•´ê°€ í•„ìš”í•œ ë¬¸ë‹µ task ë“±ì˜ ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ ëª»í•¨ì„ ì§€ì í•©ë‹ˆë‹¤.

ì´ì— ë…¼ë¬¸ì€, fine-tuning ë°©ì‹ì„ ê°œì„ í•˜ëŠ” PLMìœ¼ë¡œ BERT(Bidirectional Encoder Representations from Transformers) êµ¬ì¡°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. BERTì˜ pre-trainingì€ masked language model(MLM) ê³¼ next-sentence-prediction task(NSP)ë¥¼ í†µí•´ ì–‘ë°©í–¥ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.

# BERT

![ì¶œë ¥ ê³„ì¸µì„ ì œì™¸í•˜ê³ , ì‚¬ì „ í›ˆë ¨ê³¼ ë¯¸ì„¸ ì¡°ì • ëª¨ë‘ì—ì„œ ë™ì¼í•œ ì•„í‚¤í…ì²˜ê°€ ì‚¬ìš©ëœë‹¤. ë™ì¼í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜ëŠ” ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. ë¯¸ì„¸ ì¡°ì • ì¤‘ì—ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ë¯¸ì„¸ ì¡°ì •ë©ë‹ˆë‹¤. [CLS]ëŠ” ëª¨ë“  ì…ë ¥ ì˜ˆì œ ì•ì— ì¶”ê°€ëœ íŠ¹ìˆ˜ ê¸°í˜¸ì´ê³  [SEP]ëŠ” íŠ¹ìˆ˜ êµ¬ë¶„ í† í°(ì˜ˆ: ì§ˆë¬¸/ë‹µë³€ êµ¬ë¶„)ì…ë‹ˆë‹¤.](BERT_Pre-t/Screen_Shot_2021-11-24_at_11.39.40_PM.png)

ì¶œë ¥ ê³„ì¸µì„ ì œì™¸í•˜ê³ , ì‚¬ì „ í›ˆë ¨ê³¼ ë¯¸ì„¸ ì¡°ì • ëª¨ë‘ì—ì„œ ë™ì¼í•œ ì•„í‚¤í…ì²˜ê°€ ì‚¬ìš©ëœë‹¤. ë™ì¼í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜ëŠ” ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. ë¯¸ì„¸ ì¡°ì • ì¤‘ì—ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ë¯¸ì„¸ ì¡°ì •ë©ë‹ˆë‹¤. [CLS]ëŠ” ëª¨ë“  ì…ë ¥ ì˜ˆì œ ì•ì— ì¶”ê°€ëœ íŠ¹ìˆ˜ ê¸°í˜¸ì´ê³  [SEP]ëŠ” íŠ¹ìˆ˜ êµ¬ë¶„ í† í°(ì˜ˆ: ì§ˆë¬¸/ë‹µë³€ êµ¬ë¶„)ì…ë‹ˆë‹¤.

BERTëŠ” í¬ê²Œ pre-training ë‹¨ê³„ì™€ fine-tuning ë‹¨ê³„, ë‘ê°€ì§€ ë‹¨ê³„ë¡œ êµ¬ë¶„í•˜ë©°, ê° ë‹¨ê³„ëŠ” ëª¨ë‘ ë™ì¼í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.

Pre-training ë‹¨ê³„ì—ì„œëŠ” ë ˆì´ë¸”ë§ í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.

Fine-tuningì—ì„œëŠ” pre-trained íŒŒë¼ë¯¸í„°ë¡œ ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ë ˆì´ë¸”ë§ëœ ë°ì´í„°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

## Model Architecture

BERTì˜ êµ¬ì¡°ëŠ” ë‹¤ì¸µë ˆì´ì–´ë¡œ êµ¬ì„±ëœ ì–‘ë°©í–¥ Transformerì˜ Encoderë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ë©°, ì•„ë˜ì™€ ê°™ì€ í‘œê¸°ë¡œ ëª¨ë¸ ì†ì„±ì„ ë‚˜íƒ€ë‚´ì—ˆìŠµë‹ˆë‹¤.

- L : Layer ê°¯ìˆ˜ (ex : Transformer Block)
- H : Hidden size
- A : self-attention head ì˜ ê°¯ìˆ˜

ë…¼ë¬¸ì—ì„œëŠ” í¬ê²Œ ë‘ ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤.

- BERT-BASE : L = 12, H = 768, A = 12, ì´ íŒŒë¼ë¯¸í„° ìˆ˜ = 110M (GPT ì™€ ë™ì¼)
- BERT-LATGE : L = 24, H = 1024, A = 16, ì´ íŒŒë¼ë¯¸í„° ìˆ˜ = 240M

## Input/Output Representations

ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ë‹¤ì–‘í•œ taskì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡, ë‹¨ì¼ ë¬¸ì¥ê³¼ ìŒìœ¼ë¡œ ì´ì–´ì§„ ë¬¸ìì„ ëª¨ë‘ í•˜ë‚˜ì˜ Sequence ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œëŠ” WordPiece embedding ì„ ì‚¬ìš©í•˜ë©° 30,000ê°œì˜ token vocalbularyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

![Screen_Shot_2021-11-25_at_12.06.24_AM.png](BERT_Pre-t/Screen_Shot_2021-11-25_at_12.06.24_AM.png)

Inputì€ Token embedding + Segment embedding + Position embedding ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

- Token Embedding
    - Sequence ì˜ ì²« í† í°ì€ [CLS]í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ë‘ ë¬¸ì¥ì´ ì´ì–´ì§„ ê²½ìš°, [SEP]í† í°ìœ¼ë¡œ ë¬¸ì¥ì„ êµ¬ë¶„í•˜ë©°, ë§ˆì§€ë§‰ì—ë„ [SEP] í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        - [CLS]ëŠ” ëª¨ë“  ì…ë ¥ ì˜ˆì œ ì•ì— ì¶”ê°€ëœ íŠ¹ìˆ˜ ê¸°í˜¸ì´ê³  [SEP]ëŠ” íŠ¹ìˆ˜ êµ¬ë¶„ í† í°(ì˜ˆ: ì§ˆë¬¸/ë‹µë³€ êµ¬ë¶„)ì…ë‹ˆë‹¤.
- Segment Embedding
    - ë‘ ë¬¸ì¥ì´ ìˆì„ ë•Œ, ê°ê°ì˜ ë¬¸ì¥ì— sentence A / sentence B ì„ë² ë”©ì„ ì ìš©í•©ë‹ˆë‹¤.
- Positional Embedding
    - Transformer ì˜ ì‚¼ê°í•¨ìˆ˜ Encoding ì´ ì•„ë‹Œ, lookup table ì—ì„œ ê° positionì˜ vectorë“¤ì„ ì°¾ì•„ì„œ Position ì„ embeddingí•©ë‹ˆë‹¤.

# Pre-training BERT

ì „í†µì ì¸ left-to-right / right-to-left LM ì„ ì‚¬ìš©í•´ì„œ pre-train í•˜ëŠ” ELMo, GPTì™€ëŠ” ë‹¤ë¥´ê²Œ, BERTëŠ” 2ê°œì˜ unsupervised taskë¥¼ ì´ìš©í•´ì„œ ì‚¬ì „í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## Task #1 : Masked LM

ê¸°ì¡´ì˜ ì–¸ì–´ ëª¨ë¸ì„ Bidirectional í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²½ìš°, ê°„ì ‘ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ë ¤ëŠ” ë‹¨ì–´ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ ë˜ì–´ ì˜ˆì¸¡ ìì²´ê°€ ë¬´ì˜ë¯¸í•´ì§ˆ ìˆ˜ ìˆìŒì„ ì§€ì í•©ë‹ˆë‹¤.

BERTëŠ” ì „ì²´ Sequence ì—ì„œ 15%ì˜ í† í°ì„ ê°€ë¦¬ëŠ” Mask ë¥¼ ì¶”ê°€í•˜ì—¬ ì–‘ë°©í–¥ í•™ìŠµì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.

ë˜í•œ, ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ì´ [MASK] í† í°ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” Fine-tuningì—ë„ ì ìš©ë  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” Masking Ruleì„ ì œì‹œí•©ë‹ˆë‹¤.

- 80%ëŠ” [MASK]ë¡œ ì¹˜í™˜

```
my dog is hairy -> my dog is [MASK]
```

- 10%ëŠ” ëœë¤í•œ í† í°ìœ¼ë¡œ ì¹˜í™˜

```
my dog is hairy -> my dog is apple
```

- 10%ëŠ” ê¸°ì¡´ì˜ í† í°ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©

```
my dog is hairy -> my dog is hairy
```

ì´ë¡œ ì¸í•´ modelì€ ë‹¨ì–´ì˜ ê¸°ì›(ì›ë³¸/MASK/random changed)ì„ ì•Œì§€ ëª»í•œ ì±„ ëª¨ë“  input tokenì— ëŒ€í•´ì„œ distributional contextual representationì„ ìœ ì§€í•˜ê²Œ ë©ë‹ˆë‹¤. ë˜í•œ ì „ì²´ì˜ 1.5% í† í°ë§Œì´ ëœë¤í•˜ê²Œ ë³€ê²½ë˜ì—ˆê¸°ì—, ëª¨ë¸ì´ ì˜ëª» í•™ìŠµë  ìš°ë ¤ ë˜í•œ ì ìŒì„ ì œì‹œí•©ë‹ˆë‹¤.

![Untitled.png](BERT_Pre-t/Untitled.png)

ì´ëŸ¬í•œ ë¹„ìœ¨ì€ ìœ„ì˜ ì‹¤í—˜ì„ í†µí•´ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.

- ì „ì²´ì ìœ¼ë¡œ fine-tuning task ê°€ feature-based ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
- íŠ¹ì • Masking Rule ë§Œì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, Rule í˜¼í•©ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

ìµœì¢…ì ìœ¼ë¡œëŠ” cross-entropy loss ë¥¼ ì‚¬ìš©í•´ì„œ ê¸°ì¡´ì˜ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.

## Task #2: Next Sentence Prediction (NSP)

- Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding  he relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the entences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).
    
    <aside>
    ğŸ’¡ ë¬¸ì¥ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëª¨ë“  ë‹¨ì¼ ì–¸ì–´ ë§ë­‰ì¹˜ì—ì„œ ì‚¬ì†Œí•œ ê²ƒìœ¼ë¡œ ìƒì„±ë  ìˆ˜ ìˆëŠ” ì´í•­í™”ëœ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡ ì‘ì—…ì„ ìœ„í•´ ì‚¬ì „ í›ˆë ¨í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ê° ì‚¬ì „ í›ˆë ¨ ì˜ˆì— ëŒ€í•œ ë¬¸ì¥ Aì™€ Bë¥¼ ì„ íƒí•  ë•Œ, ì‹œê°„ì˜ 50%ëŠ” Aë¥¼ ë”°ë¥´ëŠ” ì‹¤ì œ ë‹¤ìŒ ë¬¸ì¥(IsNextë¡œ ë ˆì´ë¸”ë§ë¨)ì´ê³ , 50%ëŠ” ë§ë­‰ì¹˜(NotNextë¡œ ë ˆì´ë¸”ë§ë¨)ì˜ ë¬´ì‘ìœ„ ë¬¸ì¥ì´ë‹¤. ê·¸ë¦¼ 1ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, CëŠ” ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(NSP)ì— ì‚¬ìš©ëœë‹¤.
    
    </aside>
    

Question-answering(QA), Natural Language Interference(NLI) ë“±ì˜ taskëŠ” ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì´í•´í•´ì•¼ í•˜ëŠ” taskì…ë‹ˆë‹¤. ì¼ë°˜ì ì¸ LM ìœ¼ë¡œëŠ” í•´ë‹¹ Task ì˜ í•™ìŠµì´ ì–´ë ¤ìš°ë¯€ë¡œ, ì´ ë˜í•œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.

ë‘ ë¬¸ì¥ê³¼ ë ˆì´ë¸”ë¡œ êµ¬ì„±ëœ ë‹¤ìŒì˜ ë°ì´í„° ì…‹ìœ¼ë¡œ Binary ë¶„ë¥˜ ë¬¸ì œë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

- 50% : ì‹¤ì œë¡œ ì´ì–´ì§„ ë‘ ë¬¸ì¥ì„ ì œì‹œ : ë ˆì´ë¸” IsNext
- 50% : ê´€ê³„ê°€ ì—†ëŠ” ì„ì˜ì˜ ë¬¸ì¥ì„ ì œì‹œ : ë ˆì´ë¸” NotNext

### Pre-training data

ì‚¬ì „ í›ˆë ¨ì„ ìœ„í•´ ì‚¬ìš©í•œ corpus ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

- BooksCorpus (800M words)
- English Wikipedia (2,500M words) :  text passage ë§Œ ì‚¬ìš©í–ˆê³ , ëª©ë¡ì´ë‚˜ í‘œ ë“±ì€ ì œì™¸í•˜ì—¬ ì‚¬ìš©

ê¸´ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ Billion Word Benchmark ì™€ ê°™ì´ ì„ì¸ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

## Fine-tuning BERT

Task ë³„ ì…ë ¥ì˜ ê°œìˆ˜(ë‹¨ì¼ ë¬¸ì¥, 2ê°œì˜ ë¬¸ì¥)ì— ë”°ë¼ í•˜ë‚˜ì˜ sequence ë¥¼ ìƒì„±í•˜ì—¬ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì´í›„, íŒŒë¼ë¯¸í„°ë“¤ì„ í•´ë‹¹ taskì— ë§ê²Œ end-to-endë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

Sequence tagging ì´ë‚˜ question answering ê°™ì´ token-level task ë“¤ì˜ ê²½ìš°, ë§ˆì§€ë§‰ transformer layerì˜ token ë“¤ì„ ì‚¬ìš©í•˜ì—¬ fine-tuning í•©ë‹ˆë‹¤.

Sentence Classification, sentiment analysis ë“±ì˜ sentence-level classification task ë“¤ì€ ë§ˆì§€ë§‰ layerì˜ CLS tokenì˜ hidden stateë¥¼ fine-tuningì— ì´ìš©í•©ë‹ˆë‹¤.

Pre-trainingê³¼ ë¹„êµí–ˆì„ ë•Œ, fine-tuning ì€ ë¹ ë¥´ê²Œ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

# Experiments

BERT fine-tuningì„ ì´ìš©í•œ 11ê°œì˜ NLP taskì˜ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë“  Task ì—ì„œ State-Of-Arts ë¥¼ ë‹¬ì„±í•˜ì˜€ê³ , ê°ê°ì˜ íŠ¹ì„±ì— ë§ëŠ” í•™ìŠµ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

![Untitled (1).png](BERT_Pre-t/Untitled_(1).png)

## GLUE

GLEU benchmarkëŠ” ë‹¤ì–‘í•œ natural language understanding taskë¥¼ ìœ„í•œ ë¬¸ì¥ ë¶„ë¥˜ Task ì…ë‹ˆë‹¤. BERT ëª¨ë¸ì— ë¶„ë¥˜ë¥¼ ìœ„í•œ classification layerë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

## SQuAD v1.1

SQuAD v1.1 datasetì€ Question Answering datasetìœ¼ë¡œ, ì§ˆë¬¸ê³¼ ì§€ë¬¸ì˜ í˜•íƒœ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ ë‹µë³€ì„ ì°¾ëŠ” ê³¼ì œì…ë‹ˆë‹¤. BERT ëŠ” ì§ˆë¬¸ê³¼ ì§€ë¬¸ì„ í•˜ë‚˜ì˜ single sequence ë¡œ ë¬¶ì–´ì„œ inputìœ¼ë¡œ ë§Œë“  ë’¤, ì§€ë¬¸ì—ì„œ ì •ë‹µì´ ë  ìˆ˜ ìˆëŠ” ì˜ì—­ì„ ì°¾ëŠ” ë°©ì‹ìœ¼ë¡œ Task ë¥¼ ì „í™˜í•´ í•™ìŠµí•©ë‹ˆë‹¤.

## SQuAD v2.0

SQuAD v2.0 ì€ 1.1 ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ì§€ë¬¸ë§Œìœ¼ë¡œëŠ” ëŒ€ë‹µì´ ë¶ˆê°€ëŠ¥í•œ ì§ˆë¬¸ì´ í¬í•¨ëœ dataset ì…ë‹ˆë‹¤. BERT ëŠ” ëŒ€ë‹µì´ ë¶ˆê°€ëŠ¥í•œì§€ ì—¬ë¶€ë¥¼ CLS token ì„ ì´ìš©í•´ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

## SWAG

The Situations With Adversarial Generations (SWAG) datasetì€ ì• ë¬¸ì¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë³´ê¸°ë¡œ ì£¼ì–´ì§„ 4 ë¬¸ì¥ ì¤‘ ê°€ì¥ ì˜ ì–´ìš¸ë¦¬ëŠ” ë¬¸ì¥ì„ ì°¾ëŠ” task ì…ë‹ˆë‹¤.

Fine-tuning ì„ ìœ„í•´, ì• ë’¤ ë¬¸ì¥ì„ ì¡°í•©í•´ 4ê°œì˜ ë¬¸ì¥ì„ ìƒì„±í•˜ê³ , í•´ë‹¹ sequence ì˜ ì •ë‹µ ì—¬ë¶€ë¥¼ Classification í•˜ëŠ” Task ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

# Ablation Studies

## Effect of Pre-training Tasks

![img.png](BERT_Pre-t/img.png)

ì‚¬ì „ í›ˆë ¨ì˜ íš¨ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´, í›ˆë ¨ì„ ì œê±°í•˜ë©° ì‹¤í—˜í•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

- No NSP : masked LM(MLM) ìœ¼ë¡œë§Œ í•™ìŠµë˜ê³  NSPëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°
    - NLI ë¬¸ì œì—ì„œ ì„±ëŠ¥ì´ í•˜ë½
    - NSP ê°€ ë¬¸ì¥ê°„ì˜ ë…¼ë¦¬ì ì¸ êµ¬ì¡° íŒŒì•…ì— ì¤‘ìš”í•œ ì—­í• ì„ ìˆ˜í–‰í•¨ì„ ì‹œì‚¬
- LTR & No NSP : MLMì´ ì•„ë‹Œ Left-To-Right model ì„ ì‚¬ìš©í•˜ê³  NSPë„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°
    - ëª¨ë“  taskì— ëŒ€í•´ì„œ ì„±ëŠ¥ì´ ê°ì†Œí•˜ë©°, íŠ¹íˆ MRPCì™€ SQuAD ì—ì„œ í° í­ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ í™•ì¸
- LTR & No NSP + BiLSTM : BiLSTM ì„ ì¶”ê°€í•˜ì—¬ ì–‘ë°©í–¥ì„±ì„ ì œê³µ
    - BiLSTM ì´ ì—†ëŠ” ê²½ìš°ì— ë¹„í•´ ë¯¸ë¯¸í•˜ê²Œ ì„±ëŠ¥ì´ ìƒìŠ¹í•˜ì§€ë§Œ, MLM ì— ë¹„í•´ì„œëŠ” ë–¨ì–´ì§
    - MLMì´ bi-directionality ê°€ ë” ê°•í•¨ì„ ì‹œì‚¬

## Effect of Model size

![img (1).png](BERT_Pre-t/img_(1).png)

ëª¨ë¸ì˜ í¬ê¸°ê°€ fine-tuning ì •í™•ë„ì— ì–´ë– í•œ ì˜í–¥ì„ ì£¼ëŠ”ì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

- ëª¨ë¸ì´ ì»¤ì§ˆìˆ˜ë¡, Pre-training ì˜ ì •í™•ë„ê°€ ìƒìŠ¹
- downstream task ê°€ ì‘ì€ ìŠ¤ì¼€ì¼ì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•  ë•Œì—ë„ ì„±ëŠ¥ì´ ìƒìŠ¹

## Feature-based Approach with BERT

BERTë¥¼ ELMo ì™€ ê°™ì€ feature-based ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ê³¼ ê·¸ ì¥ì ì„ ì œì‹œí•©ë‹ˆë‹¤.

- task-specific model ì˜ ì¶”ê°€ë¡œ Transformer encoder ë§Œìœ¼ë¡œ ìˆ˜í–‰ ë¶ˆê°€ëŠ¥í•œ taskì— ì ìš© ê°€ëŠ¥
- Update parameter ê°ì†Œë¡œ í•™ìŠµ ë¹„ìš© ì ˆê°

![img (2).png](BERT_Pre-t/img_(2).png)

CoNLL-2003 Named Entity Recognition task ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

BERT ë ˆì´ì–´ì˜ ì¼ë¶€ activation ì— Bi-LSTMì„ ë¶€ì°©ì‹œì¼œ í•´ë‹¹ ë ˆì´ì–´ë§Œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ Feature-based approach ë¥¼ êµ¬í˜„í•˜ì˜€ê³ , ê¸°ì¡´ ELMo ì— ë¹„í•´ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

ë˜í•œ Fine-Tuning ë°©ë²•ë§Œì„ ì‚¬ìš©í•˜ì˜€ì„ ë•Œë„, SOTA ì— ê·¼ì ‘í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤.

![Screen_Shot_2021-11-24_at_11.41.53_PM.png](BERT_Pre-t/Screen_Shot_2021-11-24_at_11.41.53_PM.png)

ì°¸ì¡° : 

[https://www.youtube.com/watch?v=30SvdoA6ApE](https://www.youtube.com/watch?v=30SvdoA6ApE)

# Conclusion

- RNNì€ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤ë©´ TransformerëŠ” í•œë²ˆì— ê³„ì‚°.
- BERTëŠ” Transformerë¥¼ ì´ìš©í•´ì„œ ì–‘ë°©í–¥ì˜ ë¬¸ë§¥ì„ ìˆ«ìì˜ í˜•íƒœë¡œ ë°”ê¿”ì£¼ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ë‹¤.